{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to process data to analyze text, numbers and symbols for particular regular expressions that can be useful for data cleaning, representation, and decision making analyses. Let's start performing some data cleaning filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose we will first use the NLTK library characteristics (Natural Language Toolkit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Regular expressions and examples\n",
    "Data cleaning:\n",
    "    Tokenining\n",
    "    Removing punctuation\n",
    "    Stemming and Lemmatizing\n",
    "    Removing tags\n",
    "Text representation\n",
    "        TF-IDF: Term frequencies (counter)\n",
    "        Vector normalization\n",
    "        Feature weighting (Inverse Document Frequency)\t\n",
    "        Sklearn implementation\n",
    "Learning text representations\n",
    "        Stopwords\n",
    "        Bag of Words\n",
    "        n-grams\n",
    "        Training a (naive Bayes) Classifier with NLTK: film critiques example\n",
    "        Training a (naive Bayes) Classifier with TextBlob: A Tweet Sentiment Analyzer\n",
    "        Pattern module</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing text into bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK makes it easy to convert documents-as-strings into word-vectors, a process called tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'wo', \"n't\", 'be', 'very', 'interesting', ',', 'I', \"'m\", 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*very', 'simple*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print (tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation can help with tokenizers, but once you've done that, there's no reason to keep it around. There are tons of ways to remove punctuation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.escape: Return string with all non-alphanumerics backslashed; this is useful if you want to match an arbitrary literal string that may have regular expression metacharacters in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'wo', \"n't\", 'be', 'very', 'interesting', ',', 'I', \"'m\", 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*very', 'simple*', 'data', '.']]\n",
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "print (re.escape(string.punctuation))\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    \n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print (tokenized_docs)    \n",
    "print (tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation symbols are removed, and those words containing a punctuation symbol are keeped and marked with an initial 'u'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have taken linguistics, you may be familiar with morphology. This is the belief that words have a root form. If you want to get to the basic term meaning of the word, you can try applying a stemmer or lemmatizer. Here are three very popular methods ready to go right out of the NLTK box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n",
      "[['here', 'are', 'some', 'veri', 'simpl', 'basic', 'sentenc'], ['they', 'wo', 'nt', 'be', 'veri', 'interest', 'I', 'm', 'afraid'], ['the', 'point', 'of', 'these', 'exampl', 'is', 'to', 'learn', 'how', 'basic', 'text', 'clean', 'work', 'on', 'veri', 'simpl', 'data']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "       # final_doc.append(snowball.stem(word)) # requires 'corpora/wordnet' -> nltk.download()\n",
    "       # final_doc.append(wordnet.lemmatize(word)) # requires 'corpora/wordnet' -> nltk.download()\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print (tokenized_docs_no_punctuation)\n",
    "print (preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML entities and tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have it already implemented in NLTK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>While many of the stories tugged <a> at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def clean_html(html):\n",
    "    \"\"\"\n",
    "    Copied from NLTK package.\n",
    "    Remove HTML markup from the given string.\n",
    "\n",
    "    :param html: the HTML string to be cleaned\n",
    "    :type html: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    # First we remove inline JavaScript/CSS:\n",
    "    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n",
    "    # Next we can remove the remaining tags:\n",
    "    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
    "    # Finally, we deal with whitespace\n",
    "    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "test_string =\"<p>While many of the stories tugged <a> at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\"\n",
    "print (test_string)\n",
    "clean_html(test_string) \n",
    "\n",
    "#Use from bs4 import BeautifulSoup  : for improved versions of tag cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Space Model of text: TF-IDF (Term Frequency - Inverse Distance Frequency) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once text is analyzed based on regular expressions and cleaned by filtering using some of the previous tools, we can proceed to represent it in order to perform posterior analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We need to start thinking about how to translate collections of texts into quantifiable phenomena.  The easiest way to start is to think about word frequencies.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with histograms of visual words representation:\n",
    "\n",
    "<img src=\"VisualWords.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic term frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review how to get a count of terms per document: a term frequency vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Julie', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Linda', 1)])\n",
      "dict_items([('Jane', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Julie', 1), ('loves', 1)])\n",
      "dict_items([('He', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('baseball', 1)])\n"
     ]
    }
   ],
   "source": [
    "#examples taken from here: http://stackoverflow.com/a/1750187\n",
    "\n",
    "mydoclist = ['Julie loves me more than Linda loves me',\n",
    "'Jane likes me more than Julie loves me',\n",
    "'He likes basketball more than baseball']\n",
    "\n",
    "#mydoclist = ['sun sky bright', 'sun sun bright']\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] +=1\n",
    "    print (tf.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, we've introduced a new Python object called a Counter.  Counters are only in Python 2.7 and higher.  They're neat because they allow you to perform this exact kind of function; counting things in a loop.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.\n",
    "\n",
    "Elements are counted from an iterable or initialized from another mapping (or counter):\n",
    "\n",
    "<code>\n",
    "c = Counter()                           # a new, empty counter\n",
    "c = Counter('gallahad')                 # a new counter from an iterable\n",
    "</code>\n",
    "\n",
    "Counter objects have a dictionary interface except that they return a zero count for missing items instead of raising a KeyError:\n",
    "\n",
    "<code>\n",
    "c = Counter(['eggs', 'ham'])\n",
    "c['bacon']                              # count of a missing element is zero\n",
    "0\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's call this a first stab at representing documents quantitatively, just by their word counts (also thinking that we may have previously filtered and cleaned the text using previous approaches).  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [than, me, Jane, baseball, Julie, basketball, likes, Linda, loves, He, more]\n",
      "The doc is \"Julie loves me more than Linda loves me\"\n",
      "The tf vector for Document 1 is [1, 2, 0, 0, 1, 0, 0, 1, 2, 0, 1]\n",
      "The doc is \"Jane likes me more than Julie loves me\"\n",
      "The tf vector for Document 2 is [1, 2, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "The doc is \"He likes basketball more than baseball\"\n",
      "The tf vector for Document 3 is [1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "All combined, here is our master document term matrix: \n",
      "[[1, 2, 0, 0, 1, 0, 0, 1, 2, 0, 1], [1, 2, 1, 0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "    \n",
    "def build_lexicon(corpus): # define a set with all possible words included in all the sentences or \"corpus\"\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def tf(term, document):\n",
    "  return freq(term, document)\n",
    "\n",
    "def freq(term, document):\n",
    "  return document.split().count(term)\n",
    "\n",
    "vocabulary = build_lexicon(mydoclist)\n",
    "\n",
    "doc_term_matrix = []\n",
    "print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "for doc in mydoclist:\n",
    "    print ('The doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    print ('The tf vector for Document %d is [%s]' % ((mydoclist.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "print ('All combined, here is our master document term matrix: ')\n",
    "print (doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that seems reasonable enough. If any of you have any experience with machine learning, what you've just seen is the creation of a feature space. Now every document is in the same feature space, meaning that we can represent the entire corpus in the same dimensional space without having lost too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing vectors to L2 Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Once you've got your data in the same feature space, you can start applying some machine learning methods; classifying, clustering, and so on.  But actually, we've got a few problems.  Words aren't all equally informative.</p>\n",
    "<p>If words appear too frequently in a single document, they're going to muck up our analysis.  We want to perform some scaling of each of these term frequency vectors into something a bit more representative.  In other words, we need to do some <strong>vector normalizing</strong>.</p>\n",
    "<p>One possibility is to ensure that the L2 norm of each vector is equal to 1.  Here's some code that shows how this is done.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old document term matrix: \n",
      "[[1 2 0 0 1 0 0 1 2 0 1]\n",
      " [1 2 1 0 1 0 1 0 1 0 1]\n",
      " [1 0 0 1 0 1 1 0 0 1 1]]\n",
      "\n",
      "A document term matrix with row-wise L2 norms of 1:\n",
      "[[ 0.28867513  0.57735027  0.          0.          0.28867513  0.          0.\n",
      "   0.28867513  0.57735027  0.          0.28867513]\n",
      " [ 0.31622777  0.63245553  0.31622777  0.          0.31622777  0.\n",
      "   0.31622777  0.          0.31622777  0.          0.31622777]\n",
      " [ 0.40824829  0.          0.          0.40824829  0.          0.40824829\n",
      "   0.40824829  0.          0.          0.40824829  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "\n",
    "print ('A regular old document term matrix: ') \n",
    "print (np.matrix(doc_term_matrix))\n",
    "print ('\\nA document term matrix with row-wise L2 norms of 1:')\n",
    "print (np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can see immediately that we've scaled down vectors such that each element is between [0, 1], without losing too much valuable information.</p>\n",
    "<p>Why would we care about this kind of normalizing?  Think about it this way; if you wanted to make a document seem more related to a particular topic than it actually was, you might try boosting the likelihood of its inclusion into a topic by repeating the same word over and over and over again.  Frankly, at a certain point, we're getting a diminishing return on the informative value of the word.  So we need to scale down words that appear too frequently in a document.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"idf-frequency-weighting\">IDF frequency weighting</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>But we're still not there yet.  Just as all words aren't equally valuable <em>within</em> a document, not all words are valuable across <em>all documents</em>.  We can try reweighting every word by its <strong>inverse document frequency</strong>. Let's see what's involved in that.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [than, me, Jane, baseball, Julie, basketball, likes, Linda, loves, He, more]\n",
      "The inverse document frequency vector is [0.000000, 0.405465, 1.098612, 1.098612, 0.405465, 1.098612, 0.405465, 1.098612, 0.405465, 1.098612, 0.000000]\n"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount +=1\n",
    "    return doccount \n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / (float(df)) )\n",
    "\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "\n",
    "print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "print ('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we have a general sense of information values per term in our vocabulary, accounting for their relative frequency across the entire corpus.  Recall that this is an inverse!  The lower the value, the more frequent it is.</p>\n",
    "<p>To get TF-IDF weighted word vectors, we have to perform the simple calculation of tf * idf.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.40546511  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          1.09861229  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          1.09861229  0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.40546511  0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          1.09861229\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.40546511  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   1.09861229  0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.40546511  0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          1.09861229  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "print (my_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have converted our IDF vector into a matrix of size BxB, where the diagonal is the IDF vector. That means we can perform now multiply every term frequency vector by the inverse document frequency matrix. Then to make sure we are also accounting for words that appear too frequently within documents, we'll normalize each document using L2 norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'than', 'me', 'Jane', 'baseball', 'Julie', 'basketball', 'likes', 'Linda', 'loves', 'He', 'more'}\n",
      "[[ 0.          0.49474872  0.          0.          0.24737436  0.          0.\n",
      "   0.67026363  0.49474872  0.          0.        ]\n",
      " [ 0.          0.52812101  0.71547492  0.          0.2640605   0.\n",
      "   0.2640605   0.          0.2640605   0.          0.        ]\n",
      " [ 0.          0.          0.          0.56467328  0.          0.56467328\n",
      "   0.20840411  0.          0.          0.56467328  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tfidf = []\n",
    "\n",
    "#performing tf-idf matrix multiplication\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "\n",
    "#normalizing\n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "                                    \n",
    "print (vocabulary)\n",
    "print (np.matrix(doc_term_matrix_tfidf_l2)) # np.matrix() just to make it easier to look at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now let's see an efficient implementation of the previous approach using scikits-learn, which ensures that you don't have to worry about the efficiency of all the previous steps.</p>\n",
    "<p><strong>NOTE</strong>: The values you get from the <code>TfidfVectorizer/TfidfTransformer</code> will be different than what we have computed by hand. This is because scikits-learn uses an adapted version of Tfidf to deal with divide-by-zero errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'julie': 4, 'loves': 7, 'me': 8, 'more': 9, 'than': 10, 'linda': 6, 'jane': 3, 'likes': 5, 'he': 2, 'basketball': 1, 'baseball': 0}\n",
      "[[ 0.          0.          0.          0.          0.28945906  0.\n",
      "   0.38060387  0.57891811  0.57891811  0.22479078  0.22479078]\n",
      " [ 0.          0.          0.          0.41715759  0.3172591   0.3172591\n",
      "   0.          0.3172591   0.6345182   0.24637999  0.24637999]\n",
      " [ 0.48359121  0.48359121  0.48359121  0.          0.          0.36778358\n",
      "   0.          0.          0.          0.28561676  0.28561676]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
    "print (\"Vocabulary:\", count_vectorizer.vocabulary_)\n",
    "\n",
    "# print term_freq_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix)\n",
    "\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "print (tf_idf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.28945906  0.\n",
      "   0.38060387  0.57891811  0.57891811  0.22479078  0.22479078]\n",
      " [ 0.          0.          0.          0.41715759  0.3172591   0.3172591\n",
      "   0.          0.3172591   0.6345182   0.24637999  0.24637999]\n",
      " [ 0.48359121  0.48359121  0.48359121  0.          0.          0.36778358\n",
      "   0.          0.          0.          0.28561676  0.28561676]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "\n",
    "print (tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can fit new observations into this vocabulary space like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'julie': 4, 'loves': 7, 'me': 8, 'more': 9, 'than': 10, 'linda': 6, 'jane': 3, 'likes': 5, 'he': 2, 'basketball': 1, 'baseball': 0}\n",
      "[[ 0.57735027  0.57735027  0.57735027  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.68091856  0.          0.          0.51785612  0.51785612\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.62276601  0.          0.          0.62276601  0.          0.          0.\n",
      "   0.4736296   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "new_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\n",
    "new_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\n",
    "print (tfidf_vectorizer.vocabulary_)\n",
    "print (new_term_freq_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we didn't get words like 'watches' in the new_term_freq_matrix. That's because we trained the object on the documents in mydoclist, and that word doesn't appear in the vocabulary from that corpus. In other words, it's out of the lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Learning text representations\">Learning text representations</h2>\n",
    "<ul>\n",
    "<li>When using NLTK classifier, it is noted that the classifer expects <code>dict</code> style feature sets, so we need to transform the text into a <code>dict</code>.</li>\n",
    "<li>We can use previous TF-IDF. Let's see another implementation: Bag of Words.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, write a Feature extractor (the following is taken from nltk-trainer package)\n",
    "\n",
    "# download featx.py (written by Perkins)\n",
    "\n",
    "import math\n",
    "from nltk import probability\n",
    "\n",
    "def bag_of_words(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "def bag_of_words_in_set(words, wordset):\n",
    "        return bag_of_words(set(words) & wordset)\n",
    "    \n",
    "def word_counts(words):\n",
    "        return dict(probability.FreqDist((w for w in words)))\n",
    "\n",
    "def word_counts_in_set(words, wordset):\n",
    "        return word_counts((w for w in words if w in wordset))\n",
    "\n",
    "def train_test_feats(label, instances, featx=bag_of_words, fraction=0.75):\n",
    "        labeled_instances = [(featx(i), label) for i in instances]\n",
    "        \n",
    "        if fraction != 1.0:\n",
    "                l = len(instances)\n",
    "                cutoff = int(math.ceil(l * fraction))\n",
    "                return labeled_instances[:cutoff], labeled_instances[cutoff:]\n",
    "        else:\n",
    "                return labeled_instances, labeled_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'awesome': True, 'is': True, 'this': True}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "bag_of_words(['this', 'is', 'awesome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'awesome': True, 'is': True}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "bag_of_words_not_in_set(['this','is','awesome'],['this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'awesome': True}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile = 'english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "bag_of_non_stopwords(['this','is','awesome'])\n",
    "\n",
    "#nltk.download()   # try to use this is you need some package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download()   # try to use this is you need some package\n",
    "from nltk.corpus import stopwords\n",
    "print (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"bi-gram\">Bi-gram and n-grams</h3>\n",
    "<ul>\n",
    "<li>It is sometimes useful to take <em>significant</em> bi-grams into the bag-of-word model. Note that this example can be extended to n-grams.</li>\n",
    "\n",
    "<li>In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.</li>\n",
    "\n",
    "<li>An n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\". Larger sizes are sometimes referred to by the value of n, e.g., \"four-gram\", \"five-gram\", and so on.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 33, in __main__.bag_of_bigrams_words\n",
      "Failed example:\n",
      "    bag_of_bigrams_words(['the', 'quick', 'brown', 'fox'])\n",
      "Expected:\n",
      "    {'brown': True, ('brown', 'fox'): True, ('the', 'quick'): True, 'fox': True, ('quick', 'brown'): True, 'quick': True, 'the': True}\n",
      "Got:\n",
      "    {'the': True, 'quick': True, 'brown': True, 'fox': True, ('brown', 'fox'): True, ('quick', 'brown'): True, ('the', 'quick'): True}\n",
      "**********************************************************************\n",
      "File \"__main__\", line 25, in __main__.bag_of_non_stopwords\n",
      "Failed example:\n",
      "    bag_of_non_stopwords(['the', 'quick', 'brown', 'fox'])\n",
      "Expected:\n",
      "    {'quick': True, 'brown': True, 'fox': True}\n",
      "Got:\n",
      "    {'quick': True, 'fox': True, 'brown': True}\n",
      "**********************************************************************\n",
      "File \"__main__\", line 11, in __main__.bag_of_words\n",
      "Failed example:\n",
      "    bag_of_words(['the', 'quick', 'brown', 'fox'])\n",
      "Expected:\n",
      "    {'quick': True, 'brown': True, 'the': True, 'fox': True}\n",
      "Got:\n",
      "    {'the': True, 'quick': True, 'brown': True, 'fox': True}\n",
      "**********************************************************************\n",
      "File \"__main__\", line 18, in __main__.bag_of_words_not_in_set\n",
      "Failed example:\n",
      "    bag_of_words_not_in_set(['the', 'quick', 'brown', 'fox'], ['the'])\n",
      "Expected:\n",
      "    {'quick': True, 'brown': True, 'fox': True}\n",
      "Got:\n",
      "    {'quick': True, 'fox': True, 'brown': True}\n",
      "**********************************************************************\n",
      "4 items had failures:\n",
      "   1 of   1 in __main__.bag_of_bigrams_words\n",
      "   1 of   1 in __main__.bag_of_non_stopwords\n",
      "   1 of   1 in __main__.bag_of_words\n",
      "   1 of   1 in __main__.bag_of_words_not_in_set\n",
      "***Test Failed*** 4 failures.\n"
     ]
    }
   ],
   "source": [
    "#featx.py file \n",
    "\n",
    "import collections\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def bag_of_words(words):\n",
    "\t'''\n",
    "\t>>> bag_of_words(['the', 'quick', 'brown', 'fox'])\n",
    "\t{'quick': True, 'brown': True, 'the': True, 'fox': True}\n",
    "\t'''\n",
    "\treturn dict([(word, True) for word in words])\n",
    "\n",
    "def bag_of_words_not_in_set(words, badwords):\n",
    "\t'''\n",
    "\t>>> bag_of_words_not_in_set(['the', 'quick', 'brown', 'fox'], ['the'])\n",
    "\t{'quick': True, 'brown': True, 'fox': True}\n",
    "\t'''\n",
    "\treturn bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile='english'):\n",
    "\t'''\n",
    "\t>>> bag_of_non_stopwords(['the', 'quick', 'brown', 'fox'])\n",
    "\t{'quick': True, 'brown': True, 'fox': True}\n",
    "\t'''\n",
    "\tbadwords = stopwords.words(stopfile)\n",
    "\treturn bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "\t'''\n",
    "\t>>> bag_of_bigrams_words(['the', 'quick', 'brown', 'fox'])\n",
    "\t{'brown': True, ('brown', 'fox'): True, ('the', 'quick'): True, 'fox': True, ('quick', 'brown'): True, 'quick': True, 'the': True}\n",
    "\t'''\n",
    "\tbigram_finder = BigramCollocationFinder.from_words(words)\n",
    "\tbigrams = bigram_finder.nbest(score_fn, n)\n",
    "\treturn bag_of_words(words + bigrams)\n",
    "\n",
    "def bag_of_words_in_set(words, goodwords):\n",
    "\treturn bag_of_words(set(words) & set(goodwords))\n",
    "\n",
    "def label_feats_from_corpus(corp, feature_detector=bag_of_words):\n",
    "\tlabel_feats = collections.defaultdict(list)\n",
    "\t\n",
    "\tfor label in corp.categories():\n",
    "\t\tfor fileid in corp.fileids(categories=[label]):\n",
    "\t\t\tfeats = feature_detector(corp.words(fileids=[fileid]))\n",
    "\t\t\tlabel_feats[label].append(feats)\n",
    "\t\n",
    "\treturn label_feats\n",
    "\n",
    "def split_label_feats(lfeats, split=0.75):\n",
    "\ttrain_feats = []\n",
    "\ttest_feats = []\n",
    "\t\n",
    "\tfor label, feats in lfeats.items():\n",
    "\t\tcutoff = int(len(feats) * split)\n",
    "\t\ttrain_feats.extend([(feat, label) for feat in feats[:cutoff]])\n",
    "\t\ttest_feats.extend([(feat, label) for feat in feats[cutoff:]])\n",
    "\t\n",
    "\treturn train_feats, test_feats\n",
    "\n",
    "def high_information_words(labelled_words, score_fn=BigramAssocMeasures.chi_sq, min_score=5):\n",
    "\tword_fd = FreqDist()\n",
    "\tlabel_word_fd = ConditionalFreqDist()\n",
    "\t\n",
    "\tfor label, words in labelled_words:\n",
    "\t\tfor word in words:\n",
    "\t\t\tword_fd.inc(word)\n",
    "\t\t\tlabel_word_fd[label].inc(word)\n",
    "\t\n",
    "\tn_xx = label_word_fd.N()\n",
    "\thigh_info_words = set()\n",
    "\t\n",
    "\tfor label in label_word_fd.conditions():\n",
    "\t\tn_xi = label_word_fd[label].N()\n",
    "\t\tword_scores = collections.defaultdict(int)\n",
    "\t\t\n",
    "\t\tfor word, n_ii in label_word_fd[label].items():\n",
    "\t\t\tn_ix = word_fd[word]\n",
    "\t\t\tscore = score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
    "\t\t\tword_scores[word] = score\n",
    "\t\t\n",
    "\t\tbestwords = [word for word, score in word_scores.items() if score >= min_score]\n",
    "\t\thigh_info_words |= set(bestwords)\n",
    "\t\n",
    "\treturn high_info_words\n",
    "\n",
    "def reuters_high_info_words(score_fn=BigramAssocMeasures.chi_sq):\n",
    "\tlabeled_words = []\n",
    "\t\n",
    "\tfor label in reuters.categories():\n",
    "\t\tlabeled_words.append((label, reuters.words(categories=[label])))\n",
    "\t\n",
    "\treturn high_information_words(labeled_words, score_fn=score_fn)\n",
    "\n",
    "def reuters_train_test_feats(feature_detector=bag_of_words):\n",
    "\ttrain_feats = []\n",
    "\ttest_feats = []\n",
    "\t\n",
    "\tfor fileid in reuters.fileids():\n",
    "\t\tif fileid.startswith('training'):\n",
    "\t\t\tfeatlist = train_feats\n",
    "\t\telse: # fileid.startswith('test')\n",
    "\t\t\tfeatlist = test_feats\n",
    "\t\t\n",
    "\t\tfeats = feature_detector(reuters.words(fileid))\n",
    "\t\tlabels = reuters.categories(fileid)\n",
    "\t\tfeatlist.append((feats, labels))\n",
    "\t\n",
    "\treturn train_feats, test_feats\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\timport doctest\n",
    "\tdoctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': True,\n",
       " 'is': True,\n",
       " 'an': True,\n",
       " 'incredible': True,\n",
       " 'place': True,\n",
       " ('an', 'incredible'): True,\n",
       " ('incredible', 'place'): True,\n",
       " ('is', 'an'): True,\n",
       " ('this', 'is'): True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from featx import bag_of_bigrams_words\n",
    "bag_of_bigrams_words(['this','is','an','incredible','place'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"training-a-naive-bayes-classifier\">Training a (naive Bayes) Classifier with NLTK: film critiques example</h2>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">\n",
    "</div>\n",
    "<div class=\"inner_cell\">\n",
    "<div class=\"text_cell_render border-box-sizing rendered_html\">\n",
    "<ul>\n",
    "<li>Once we have extracted features from text, we can train a classifier.</li>\n",
    "<li>The easiest classifer to get started is the <code>NaiveBayesClassifer</code>.<ul>\n",
    "<li>It uses <em>Bayes Theorem</em> to predict the probability that a given feature set belongs to a particular label. Recall the formula (more detail in the next classes):<pre><code>P(label|features) = P(label) * P(features|label) / P(features)\n",
    "</code></pre></li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">\n",
    "</div>\n",
    "<div class=\"inner_cell\">\n",
    "<div class=\"text_cell_render border-box-sizing rendered_html\">\n",
    "<ul>\n",
    "<li>Corpus: movie reviews corpus<ul>\n",
    "<li>each file in the corpus is composed of either positive or negative movie reviews.</li>\n",
    "<li>let's try a sentiment analysis.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()\n",
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the <code>label_feats_from_corpus()</code> function takes a <em>corpus</em>, and a <em>feature_detector function</em>, which is <code>bag_of_words()</code> by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['neg', 'pos'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfeats = label_feats_from_corpus(movie_reviews)\n",
    "lfeats.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get a mapping of <code>label:feature</code> sets, as shown in <code>lfeats</code>: <pre><code>defaultdict(&lt;type 'list'&gt;, {'neg': [{'all': True, 'concept': True, 'skip': True, 'go': True, 'seemed': True, 'suits': True, 'presents': True, 'to': True, 'sitting': True, 'very': True, 'horror': True, 'continues': True, 'every': True, 'exact': True, 'cool': True, 'entire': True, 'did': True, 'dig': True, 'flick': True, 'neighborhood': True, 'crow': True, 'street': True, 'video': True, 'further': True,.............\n",
    "</code></pre>we need to split the data into training and testing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (split = 0.75) by default\n",
    "\n",
    "train_feats, test_feats = split_label_feats(lfeats)\n",
    "len(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 1,000 pos files, 1,000 neg files, and we end up woth 1,500 labeled training instances and 500 labeled testing instances. Now we can train a NaiveBayesClassifier using its train() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "nb_classifier = NaiveBayesClassifier.train(train_feats)\n",
    "nb_classifier.labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, let's test the classifer on some made-up reviews. The classify() method takes a single argument, which should be a feature set. We can use bag_of_words() feature detector on a made-up list of words to get the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negfeat = bag_of_words(['the', 'plot', 'was', 'ludicrous'])\n",
    "nb_classifier.classify(negfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posfeat = bag_of_words(['kate', 'winslet', 'is', 'accessible'])\n",
    "nb_classifier.classify(posfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the accuracy of the classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "accuracy(nb_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the classification probability of each label, you can use the prob_classify() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5356889692412044e-08"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = nb_classifier.prob_classify(test_feats[0][0])\n",
    "probs.samples()\n",
    "# ['neg','pos']\n",
    "probs.max()\n",
    "#'pos'\n",
    "probs.prob('pos')\n",
    "#0.99999996464309127\n",
    "probs.prob('neg')\n",
    "#3.5356889692409258e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most_informative_features() method returns a list of form [(feature name, feature value)] ordered by most informative to least informative. In the case, the feature value will always be True, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('magnificent', True),\n",
       " ('outstanding', True),\n",
       " ('insulting', True),\n",
       " ('vulnerable', True),\n",
       " ('ludicrous', True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.most_informative_features(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The show_most_informative_features() method will print out the results and include the probability of a feature pair belonging to each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"training-a-naive-bayes-classifier\">Training a (naive Bayes) Classifier with TextBlob: A Tweet Sentiment Analyzer</h2>\n",
    "\n",
    "We will train a simple sentiment analyzer trained on a small dataset of fake tweets. To begin, well import the text.classifiers and create some training and test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.twitter.com/2012/a-new-barometer-for-the-election\n",
    "\n",
    "\"( Wednesday, August 1, 2012 | By Adam Sharp (@AdamS) [16:00 UTC] ) One glance at the numbers, and its easy to see why pundits are already calling 2012 the Twitter election. More Tweets are sent every two days today than had ever been sent prior to Election Day 2008  and Election Day 2008s Tweet volume represents only about six minutes of Tweets today.\"\n",
    "\n",
    "\"Each day, the Index evaluates and weighs the sentiment of Tweets mentioning Obama or Romney relative to the more than 400 million Tweets sent on all other topics. For example, a score of 73 for a candidate indicates that Tweets containing their name or account name are on average more positive than 73 percent of all Tweets.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tweetpolitics.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    (\"What an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new classifier by passing training data into the constructor for a NaiveBayesClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now classify arbitrary text using the NaiveBayesClassifier.classify(text) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify(\"Their burgers are amazing\")  # \"pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify(\"I don't like their pizza.\")  # \"neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to classify strings of text is to use TextBlob objects. You can pass classifiers into the constructor of a TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beer was amazing. But the hangover was horrible. My boss was not happy.\n"
     ]
    }
   ],
   "source": [
    "import textblob\n",
    "blob = textblob.TextBlob(\"The beer was amazing. \"\n",
    "                \"But the hangover was horrible. My boss was not happy.\", classifier=cl)\n",
    "print (blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then call the classify() method on the blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.classify()  # \"neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take advantage of TextBlobs sentence tokenization and classify each sentence indvidually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beer was amazing.\n",
      "pos\n",
      "But the hangover was horrible.\n",
      "neg\n",
      "My boss was not happy.\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print(sentence)\n",
    "    print(sentence.classify())\n",
    "# \"pos\", \"neg\", \"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our classifier by adding more training and test data. Here well add data from the movie review corpus which was downloaded with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "new_train, new_test = reviews[0:100], reviews[101:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what one of these documents looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(new_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update our classifier with the new training data using the update(new_data) method, as well as test it using the larger test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "cl.update(new_train) # it takes a while\n",
    "accuracy = cl.accuracy(test + new_test) \n",
    "print(\"Accuracy: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres the full, updated script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "Accuracy: 0.7714285714285715\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "random.seed(1)\n",
    " \n",
    "train = [\n",
    "('I love this sandwich.', 'pos'),\n",
    "('This is an amazing place!', 'pos'),\n",
    "('I feel very good about these beers.', 'pos'),\n",
    "('This is my best work.', 'pos'),\n",
    "(\"What an awesome view\", 'pos'),\n",
    "('I do not like this restaurant', 'neg'),\n",
    "('I am tired of this stuff.', 'neg'),\n",
    "(\"I can't deal with this\", 'neg'),\n",
    "('He is my sworn enemy!', 'neg'),\n",
    "('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "('The beer was good.', 'pos'),\n",
    "('I do not enjoy my job', 'neg'),\n",
    "(\"I ain't feeling dandy today.\", 'neg'),\n",
    "(\"I feel amazing!\", 'pos'),\n",
    "('Gary is a friend of mine.', 'pos'),\n",
    "(\"I can't believe I'm doing this.\", 'neg')\n",
    "]\n",
    " \n",
    "cl = NaiveBayesClassifier(train)\n",
    "accuracy = cl.accuracy(test)\n",
    "print(\"Accuracy: {0}\".format(accuracy))\n",
    "\n",
    "# Grab some movie review data\n",
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "           for category in movie_reviews.categories()\n",
    "           for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(reviews)\n",
    "new_train, new_test = reviews[0:100], reviews[101:200]\n",
    " \n",
    "# Update the classifier with the new training data\n",
    "cl.update(new_train)\n",
    " \n",
    "# Compute accuracy\n",
    "accuracy = cl.accuracy(test + new_test)\n",
    "print(\"Accuracy: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern python module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern is a web mining module for the Python programming language. It bundles tools for data mining (Google + Twitter + Wikipedia API, web crawler, HTML DOM parser), natural language processing (part-of-speech taggers, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules:\n",
    "    \n",
    "    pattern.web\n",
    "    pattern.db\n",
    "    pattern.en | es | de | fr | it | nl\n",
    "    pattern.search\n",
    "    pattern.vector\n",
    "    pattern.graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older Python versions -> for new ones python-twitter 3.3\n",
    "# pip install pattern\n",
    "#\n",
    "#from pattern.web import Twitter3\n",
    "#t = Twitter3()\n",
    "#i = None\n",
    "#for j in range(3):\n",
    "#    print (j)\n",
    "#    for tweet in t.search('notebook', start=i, count=10):\n",
    "#        print (tweet)\n",
    "#        print (tweet.text)\n",
    "#        print\n",
    "#        i = tweet.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a binary problem to be learnt by a binary naive bayes classifier. For this purpose define a set of text examples from two\n",
    "binary categories:\n",
    "\n",
    "$\\bullet$Design a corpus or load it from already existing examples (or data from previous classes)\n",
    "\n",
    "$\\bullet$Define the positive and negative sets\n",
    "\n",
    "$\\bullet$Train the classifier (define a proper partition of train and test data)\n",
    "\n",
    "$\\bullet$Estimate the performance of your trained classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of some useful functions\n",
    "\n",
    "<img src=\"summary.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
